{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a52bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "import os\n",
    "# Add parent directory to sys.path\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "import edge_cases_discovery\n",
    "importlib.reload(edge_cases_discovery)\n",
    "from edge_cases_discovery import edge_cases_descriptions_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7720d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_cases_descriptions_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f7be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "import json\n",
    "import os\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "# Add parent directory to sys.path\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "import edge_cases_discovery\n",
    "importlib.reload(edge_cases_discovery)\n",
    "from edge_cases_discovery import process_test_files\n",
    "from edge_cases_discovery import generate_domains_mistral\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2e72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "\n",
    "MODEL_PATH = os.getenv(\"MISTRAL_MODEL_PATH\")\n",
    "\n",
    "# Test loading Llama 2 7B with 4-bit quantization\n",
    "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n",
    "    device_map='auto',\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "\n",
    "\n",
    "print('âœ… Your Legion 5 Pro can handle it!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3094591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_directory = \"../data/edge_case_test_sets\"\n",
    "\n",
    "process_test_files(\n",
    "    test_set_directory,\n",
    "    mistral_model,\n",
    "    mistral_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5370361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"A literary magazine publishing erotic fiction and poetry, focusing on the artistic and emotional aspects of intimacy.\"\n",
    "generate_domains_mistral(mistral_model, mistral_tokenizer, description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
